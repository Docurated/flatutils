{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "\n",
    "We create two files, each of which is larger than available RAM, and then join them. We try this using three methods:\n",
    "\n",
    "* Tab-delimited text file\n",
    "* numpy ndarray\n",
    "* Pandas HDF5 table\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "NUM_ROWS = 120 * 1000 * 1000\n",
    "TEMP_DIR = os.path.join(os.environ['TMPDIR'], 'flatutils')\n",
    "\n",
    "if not os.path.exists(TEMP_DIR):\n",
    "    os.mkdir(TEMP_DIR)\n",
    "\n",
    "def file_name(index):\n",
    "    return os.path.join(TEMP_DIR, 'file{0}.txt'.format(index))\n",
    "\n",
    "def make_file(index):\n",
    "    fn = file_name(index)\n",
    "    print(\"making {0}\".format(fn))\n",
    "    with open(fn, 'w') as f:\n",
    "        for i in range(NUM_ROWS + 1):\n",
    "            if i > 0 and i % 40000000 == 0:\n",
    "                print(\"At row {0}\".format(i))\n",
    "            arith_inv_i = NUM_ROWS - i\n",
    "            f.write(\"{0}\\t\" \\\n",
    "                    \"abcdefghijlmnopqrstuv{0}\\t\" \\\n",
    "                    \"abcdefghijlmnopqrs{0}\\t\" \\\n",
    "                    \"abcdefghijlmn{0}\\n\".format(arith_inv_i))\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making /var/pg_dumps/temp/flatutils/file0.txt\n",
      "At row 40000000\n",
      "At row 80000000\n",
      "At row 120000000\n",
      "CPU times: user 2min 40s, sys: 7.98 s, total: 2min 48s\n",
      "Wall time: 2min 49s\n",
      "making /var/pg_dumps/temp/flatutils/file1.txt\n",
      "At row 40000000\n",
      "At row 80000000\n",
      "At row 120000000\n",
      "CPU times: user 2min 40s, sys: 8.2 s, total: 2min 48s\n",
      "Wall time: 2min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10595555652"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time make_file(0)\n",
    "%time make_file(1)\n",
    "\n",
    "os.stat(file_name(0)).st_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for each approach is to join the files together by their third column.\n",
    "\n",
    "## Tab-delimited text file approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 42s, sys: 57.3 s, total: 18min 39s\n",
      "Wall time: 22min 30s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/etc/etl/flatutils/flatutils')\n",
    "\n",
    "from flatutils import Field, Schema, FlatFile, FIELD_INT, FIELD_STRING\n",
    "\n",
    "schema = Schema([Field(\"col0\", FIELD_INT, 0), \n",
    "                 Field(\"col1\", FIELD_STRING, 1),\n",
    "                 Field(\"col2\", FIELD_STRING, 2),\n",
    "                 Field(\"col3\", FIELD_STRING, 3)])\n",
    "\n",
    "file0 = FlatFile(file_name(0), schema)\n",
    "file1 = FlatFile(file_name(0), schema)\n",
    "\n",
    "%time sorted0 = file0.output_sorted(file_name(2), \"col2\", temp_dir=TEMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 42s, sys: 56.9 s, total: 18min 39s\n",
      "Wall time: 22min 31s\n"
     ]
    }
   ],
   "source": [
    "%time sorted1 = file1.output_sorted(file_name(3), \"col2\", temp_dir=TEMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20000000 total rows\n",
      "Processed 20000000 joined rows\n",
      "Processed 40000000 total rows\n",
      "Processed 40000000 joined rows\n",
      "Processed 60000000 total rows\n",
      "Processed 60000000 joined rows\n",
      "Processed 80000000 total rows\n",
      "Processed 80000000 joined rows\n",
      "Processed 100000000 total rows\n",
      "Processed 100000000 joined rows\n",
      "Processed 120000000 total rows\n",
      "Processed 120000000 joined rows\n",
      "CPU times: user 20min 13s, sys: 20.1 s, total: 20min 33s\n",
      "Wall time: 20min 44s\n"
     ]
    }
   ],
   "source": [
    "def join():\n",
    "    rowiter0 = sorted0.iterate_rows()\n",
    "    rowiter1 = sorted1.iterate_rows()\n",
    "    row0 = next(rowiter0, None)\n",
    "    row1 = next(rowiter1, None)\n",
    "    total_count, join_count = 0, 0\n",
    "    with open(file_name(4), 'w') as outf:\n",
    "        while row0 is not None and row1 is not None:\n",
    "            val0 = row0['col2']\n",
    "            val1 = row1['col2']\n",
    "            total_count += 1\n",
    "            if val0 == val1:\n",
    "                join_count += 1\n",
    "                outf.write(\"{0}\\t{1}\\t{2}\\t{3}\".format(\n",
    "                    row0['col0'], row0['col1'], \n",
    "                    row0['col2'], row1['col3']))\n",
    "                row0 = next(rowiter0, None)\n",
    "                row1 = next(rowiter1, None)\n",
    "            elif val0 < val1:\n",
    "                row0 = next(rowiter0, None)\n",
    "            else:\n",
    "                row1 = next(rowiter1, None)\n",
    "            if total_count % 20000000 == 0:\n",
    "                print(\"Processed {0} total rows\".format(total_count))\n",
    "            if join_count % 20000000 == 0:\n",
    "                print(\"Processed {0} joined rows\".format(join_count))\n",
    "\n",
    "%time join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas approach\n",
    "In the Pandas approach, we're going to emulate the two files as being divided between different orgs, each with 6 million rows. Then we'll \"join\" each org individually using Pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.2 s, sys: 1.4 s, total: 15.6 s\n",
      "Wall time: 15.6 s\n",
      "CPU times: user 14.3 s, sys: 1.08 s, total: 15.4 s\n",
      "Wall time: 15.4 s\n",
      "CPU times: user 8 s, sys: 260 ms, total: 8.26 s\n",
      "Wall time: 8.26 s\n",
      "CPU times: user 31.1 s, sys: 764 ms, total: 31.9 s\n",
      "Wall time: 32 s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "columns = ['col0', 'col1', 'col2', 'col3']\n",
    "kwargs = dict(\n",
    "    header=None,\n",
    "    names=columns,\n",
    "    index_col=2\n",
    ")\n",
    "chunkiter0 = pd.read_table(file_name(0), **kwargs)\n",
    "chunkiter1 = pd.read_table(file_name(1), **kwargs)\n",
    "%time df0 = next(chunkiter0, None)\n",
    "%time df1 = next(chunkiter1, None)\n",
    "%time result_df = df0.join(df1, how='inner', lsuffix=\"_left\", rsuffix=\"_right\")\n",
    "%time result_df.to_csv(os.path.join(TEMP_DIR, \"joined.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
