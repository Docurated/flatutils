{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "\n",
    "We create two files, each of which is larger than available RAM, and then join them. We try this using three methods:\n",
    "\n",
    "* Tab-delimited text file\n",
    "* numpy ndarray\n",
    "* Pandas HDF5 table\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "NUM_ROWS = 120 * 1000 * 1000\n",
    "TEMP_DIR = os.path.join(os.environ['TMPDIR'], 'flatutils')\n",
    "\n",
    "if not os.path.exists(TEMP_DIR):\n",
    "    os.mkdir(TEMP_DIR)\n",
    "\n",
    "def file_name(index):\n",
    "    return os.path.join(TEMP_DIR, 'file{0}.txt'.format(index))\n",
    "\n",
    "def make_file(index):\n",
    "    fn = file_name(index)\n",
    "    print(\"making {0}\".format(fn))\n",
    "    with open(fn, 'w') as f:\n",
    "        for i in range(NUM_ROWS + 1):\n",
    "            if i > 0 and i % 40000000 == 0:\n",
    "                print(\"At row {0}\".format(i))\n",
    "            arith_inv_i = NUM_ROWS - i\n",
    "            f.write(\"{0}\\t\" \\\n",
    "                    \"abcdefghijlmnopqrstuv{0}\\t\" \\\n",
    "                    \"abcdefghijlmnopqrs{0}\\t\" \\\n",
    "                    \"abcdefghijlmn{0}\\n\".format(arith_inv_i))\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making /var/folders/h4/vtwhb_7s4c74lrk93lhr35cc0000gn/T/flatutils/file0.txt\n",
      "At row 40000000\n",
      "At row 80000000\n",
      "At row 120000000\n",
      "CPU times: user 3min 21s, sys: 13.5 s, total: 3min 34s\n",
      "Wall time: 3min 35s\n",
      "making /var/folders/h4/vtwhb_7s4c74lrk93lhr35cc0000gn/T/flatutils/file1.txt\n",
      "At row 40000000\n",
      "At row 80000000\n",
      "At row 120000000\n",
      "CPU times: user 3min 26s, sys: 13.6 s, total: 3min 39s\n",
      "Wall time: 3min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10595555652"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time make_file(0)\n",
    "%time make_file(1)\n",
    "\n",
    "os.stat(file_name(0)).st_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for each approach is to join the files together by their third column.\n",
    "\n",
    "## Tab-delimited text file approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26min 41s, sys: 1min 37s, total: 28min 18s\n",
      "Wall time: 28min 28s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/Users/aduston/dev/flatutils/flatutils')\n",
    "\n",
    "from flatutils import Field, Schema, FlatFile, FIELD_INT, FIELD_STRING\n",
    "\n",
    "schema = Schema([Field(\"col0\", FIELD_INT, 0), \n",
    "                 Field(\"col1\", FIELD_STRING, 1),\n",
    "                 Field(\"col2\", FIELD_STRING, 2),\n",
    "                 Field(\"col3\", FIELD_STRING, 3)])\n",
    "\n",
    "file0 = FlatFile(file_name(0), schema)\n",
    "file1 = FlatFile(file_name(0), schema)\n",
    "\n",
    "%time sorted0 = file0.output_sorted(file_name(2), \"col2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26min 40s, sys: 1min 36s, total: 28min 16s\n",
      "Wall time: 28min 26s\n"
     ]
    }
   ],
   "source": [
    "%time sorted1 = file1.output_sorted(file_name(3), \"col2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20000000 total rows\n",
      "Processed 20000000 joined rows\n",
      "Processed 40000000 total rows\n",
      "Processed 40000000 joined rows\n",
      "Processed 60000000 total rows\n",
      "Processed 60000000 joined rows\n",
      "Processed 80000000 total rows\n",
      "Processed 80000000 joined rows\n",
      "Processed 100000000 total rows\n",
      "Processed 100000000 joined rows\n"
     ]
    }
   ],
   "source": [
    "def join():\n",
    "    rowiter0 = sorted0.iterate_rows()\n",
    "    rowiter1 = sorted1.iterate_rows()\n",
    "    row0 = next(rowiter0, None)\n",
    "    row1 = next(rowiter1, None)\n",
    "    total_count, join_count = 0, 0\n",
    "    with open(file_name(4), 'w') as outf:\n",
    "        while row0 is not None and row1 is not None:\n",
    "            val0 = row0['col2']\n",
    "            val1 = row1['col2']\n",
    "            total_count += 1\n",
    "            if val0 == val1:\n",
    "                join_count += 1\n",
    "                outf.write(\"{0}\\t{1}\\t{2}\\t{3}\".format(\n",
    "                    row0['col0'], row0['col1'], \n",
    "                    row0['col2'], row1['col3']))\n",
    "                row0 = next(rowiter0, None)\n",
    "                row1 = next(rowiter1, None)\n",
    "            elif val0 < val1:\n",
    "                row0 = next(rowiter0, None)\n",
    "            else:\n",
    "                row1 = next(rowiter1, None)\n",
    "            if total_count % 20000000 == 0:\n",
    "                print(\"Processed {0} total rows\".format(total_count))\n",
    "            if join_count % 20000000 == 0:\n",
    "                print(\"Processed {0} joined rows\".format(join_count))\n",
    "\n",
    "%time join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas approach\n",
    "In the Pandas approach, we're going to emulate the two files as being divided between different orgs, each with 8 million rows. Then we'll \"join\" each org individually using Pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def join_dataframes():\n",
    "    columns = ['col0', 'col1', 'col2', 'col3']\n",
    "    kwargs = dict(\n",
    "        chunksize=8000000,\n",
    "        header=None,\n",
    "        names=columns,\n",
    "        index_col=2\n",
    "    )\n",
    "    chunkiter0 = pd.read_table(file_name(0), **kwargs)\n",
    "    chunkiter1 = pd.read_table(file_name(1), **kwargs)\n",
    "    df0 = next(chunkiter0, None)\n",
    "    df1 = next(chunkiter1, None)\n",
    "    count = 0\n",
    "    while df0 is not None and df1 is not None:\n",
    "        count += 1\n",
    "        print(\"count at {0}\".format(count))\n",
    "        result_df = df0.join(df1, how='inner', lsuffix=\"_left\", rsuffix=\"_right\")\n",
    "        result_df.to_csv(os.path.join(TEMP_DIR, \"joined{0}.csv\".format(count)))\n",
    "        df0 = next(chunkiter0, None)\n",
    "        df1 = next(chunkiter1, None)\n",
    "\n",
    "%time join_dataframes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
